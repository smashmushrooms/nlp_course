{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging with maximum entropy models (10 pts)\n",
    "\n",
    "In this task you will build a maximum entropy model for part-of-speech tagging. As the name suggests, our problem is all about converting a sequence of words into a sequence of part-of-speech tags. \n",
    "<img src=https://i.stack.imgur.com/6pdIT.png width=320>\n",
    "\n",
    "\n",
    "__Your man goal:__ implement the model from [the article you're given](W96-0213.pdf).\n",
    "\n",
    "Unlike previous tasks, this one gives you greater degree of freedom and less automated tests. We provide you with programming interface but nothing more.\n",
    "\n",
    "__A piece of advice:__ there's a lot of objects happening here. If you don't understand why some object is needed, find `def train` function and see how everything is linked together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types:\n",
    "# Word: str\n",
    "# Sentence: list of str\n",
    "TaggedWord = collections.namedtuple('TaggedWord', ['text', 'tag'])\n",
    "# TaggedSentence: list of TaggedWord\n",
    "# Tags: list of TaggedWord\n",
    "# TagLattice: list of Tags\n",
    "\n",
    "def read_tagged_sentences(path):\n",
    "    \"\"\"\n",
    "    Read tagged sentences from CoNLL-U file and return array of TaggedSentence (array of lists of TaggedWord).\n",
    "    \"\"\"\n",
    "    tagged_sentences = []\n",
    "    \n",
    "    with open(path, encoding='utf-8') as reader:\n",
    "        tagged_sentence = []\n",
    "        for line in reader.readlines():\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            elif line.startswith('\\n'):\n",
    "                tagged_sentences.append(tagged_sentence)\n",
    "                tagged_sentence = []\n",
    "            else:\n",
    "                line = line.split('\\t')\n",
    "                tagged_sentence.append(TaggedWord(text=line[1], tag=line[3]))\n",
    "    \n",
    "    return tagged_sentences\n",
    "\n",
    "def write_tagged_sentence(tagged_sentence, f):\n",
    "    \"\"\"\n",
    "    Write tagged sentence in CoNLL-U format to file-like object f.\n",
    "    \"\"\"\n",
    "    words, tags = zip(*tagged_sentence)\n",
    "    \n",
    "    f.write(' '.join(words))\n",
    "    f.write(' '.join(tags))\n",
    "    \n",
    "\n",
    "def read_tags(path):\n",
    "    \"\"\"\n",
    "    Read a list of possible tags from file and return the list.\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        tags = f.readlines()\n",
    "        \n",
    "    return list(map(str.strip, tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: evaluation\n",
    "\n",
    "We want you to estimate tagging quality by a simple accuracy: a fraction of tag predictions that turned out to be correct - averaged over the entire training corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types:\n",
    "TaggingQuality = collections.namedtuple('TaggingQuality', ['acc'])\n",
    "\n",
    "def tagging_quality(ref, out):\n",
    "    \"\"\"\n",
    "    Compute tagging quality and reutrn TaggingQuality object.\n",
    "    \"\"\"\n",
    "    pront(ref[0], out[0])\n",
    "    nwords = 0\n",
    "    ncorrect = 0\n",
    "    import itertools\n",
    "    for ref_sentence, out_sentence in itertools.zip_longest(ref, out):\n",
    "        for ref_word, out_word in itertools.zip_longest(ref_sentence, out_sentence):\n",
    "            nwords += 1\n",
    "            ncorrect += ref_word.tag == out_word.tag\n",
    "    return TaggingQuality(ncorrect / nwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Value and Update\n",
    "\n",
    "In order to implement two interlinked data structures: \n",
    "* __Value__ - a class that holds POS tagger's parameters. Basically an array of numbers\n",
    "* __Update__ - a class that stores updates for Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Dense object that holds parameters.\n",
    "        :param n: array length\n",
    "        \"\"\"\n",
    "        self.params = np.random.normal(0, 0.01, n)\n",
    "\n",
    "    def dot(self, update):\n",
    "        return np.dot(self.params[update.positions], update.values)\n",
    "\n",
    "    def assign(self, other):\n",
    "        \"\"\"\n",
    "        self = other\n",
    "        other is Value.\n",
    "        \"\"\"\n",
    "        self.params = other.params\n",
    "\n",
    "    def assign_mul(self, coeff):\n",
    "        \"\"\"\n",
    "        self = self * coeff\n",
    "        coeff is float.\n",
    "        \"\"\"\n",
    "        self.params *= coef\n",
    "\n",
    "    def assign_madd(self, x, coeff):\n",
    "        \"\"\"\n",
    "        self = self + x * coeff\n",
    "        x can be either Value or Update.\n",
    "        coeff is float.\n",
    "        \"\"\"\n",
    "        if isinstance(x, Value):\n",
    "            self.params += x.params * coeff\n",
    "        elif isinstance(x, Update):\n",
    "            self.params[x.positions.astype(int)] += x.values * coeff\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.params.shape[0]\n",
    "\n",
    "\n",
    "class Update:\n",
    "    \"\"\"\n",
    "    Sparse object that holds an update of parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, positions=[], values=[]):\n",
    "        \"\"\"\n",
    "        positions: array of int\n",
    "        values: array of float\n",
    "        \"\"\"\n",
    "        self.positions = np.array(positions, dtype=np.int64)\n",
    "        self.values = np.array(values)\n",
    "\n",
    "    def assign_mul(self, coeff):\n",
    "        \"\"\"\n",
    "        self = self * coeff\n",
    "        coeff: float\n",
    "        \"\"\"\n",
    "        self.values *= coeff\n",
    "\n",
    "    def assign_madd(self, update, coeff):\n",
    "        \"\"\"\n",
    "        self = self + update * coeff\n",
    "        coeff: float\n",
    "        \"\"\"\n",
    "        self.positions = np.concatenate((self.positions, update.positions))\n",
    "        self.values = np.concatenate((self.values, update.values * coeff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: Maximum Entropy POS Tagger\n",
    "_step 1 - draw an oval; step 2 - draw the rest of the owl (c)_\n",
    "\n",
    "In this secion you will implement a simple linear model to predict POS tags.\n",
    "Make sure you [read the article](W96-0213.pdf) before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types:\n",
    "Features = Update\n",
    "Hypo = collections.namedtuple('Hypo', ['prev', 'pos', 'tagged_word', 'score'])\n",
    "# prev: previous Hypo\n",
    "# pos: position of word (0-based)\n",
    "# tagged_word: tagging of source_sentence[pos]\n",
    "# score: sum of scores over edges\n",
    "\n",
    "TaggerParams = collections.namedtuple('FeatureParams', [\n",
    "    'src_window',\n",
    "    'dst_order',\n",
    "    'max_suffix',\n",
    "    'beam_size',\n",
    "    'nparams'\n",
    "    ])\n",
    "\n",
    "def h(x):\n",
    "    \"\"\"\n",
    "    Compute CityHash of any object.\n",
    "    Can be used to construct features.\n",
    "    \"\"\"\n",
    "    import mmh3\n",
    "    return mmh3.hash64(repr(x))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    \"\"\"\n",
    "    A thing that computes score and gradient for given features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self._params = Value(n)\n",
    "\n",
    "    def params(self):\n",
    "        return self._params\n",
    "\n",
    "    def score(self, features):\n",
    "        \"\"\"\n",
    "        features: Update\n",
    "        \"\"\"\n",
    "        return self._params.dot(features)\n",
    "\n",
    "    def gradient(self, features, score):\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class FeatureComputer:\n",
    "    def __init__(self, tagger_params, source_sentence):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.source_sentence = source_sentence\n",
    "\n",
    "    def compute_features(self, hypo):\n",
    "        \"\"\"\n",
    "        Compute features for a given Hypo and return Update.\n",
    "        \"\"\"\n",
    "        word = hypo.tagged_word.text.lower()\n",
    "        features = [h((hypo.tagged_word.tag, word))]\n",
    "        \n",
    "        for length in range(1, self.tagger_params.max_suffix + 1):\n",
    "            if length <= len(word):\n",
    "                features.extend([\n",
    "                    h((hypo.tagged_word.tag, 'suffix_{}_{}'.format(length, word[-length:]))),\n",
    "                    h((hypo.tagged_word.tag, 'prefix_{}_{}'.format(length, word[:length])))\n",
    "                ])\n",
    "                features.extend([\n",
    "                    h((hypo.tagged_word.tag, 'suffix_{}_{}'.format(length, \"\"))),\n",
    "                    h((hypo.tagged_word.tag, 'prefix_{}_{}'.format(length, \"\")))\n",
    "                ])\n",
    "                \n",
    "            features.extend([\n",
    "                h((hypo.tagged_word.tag, 'is_capital_{}'.format(\n",
    "                    hypo.tagged_word.text == hypo.tagged_word.text.capitalize()))),\n",
    "                h((hypo.tagged_word.tag, 'is_upper_{}'.format(hypo.tagged_word.text.isupper()))),\n",
    "                h((hypo.tagged_word.tag, 'is_number_{}'.format(None != re.search('\\d', word)))),\n",
    "                h((hypo.tagged_word.tag, 'has_hyphen_{}'.format(None != re.search('\\-', word)))),\n",
    "            ])\n",
    "            \n",
    "            new_hypo = hypo\n",
    "            prev_tags = []\n",
    "            for tag_ind in range(self.tagger_params.dst_order):\n",
    "                new_hypo = new_hypo.prev if new_hypo else None\n",
    "    \n",
    "                if new_hypo is None:\n",
    "                    prev_tags.append(None)\n",
    "                else:\n",
    "                    prev_tags.append(new_hypo.tagged_word.tag)\n",
    "                \n",
    "                features.append(h((hypo.tagged_word.tag, 'prev_{}_tags_{}'.format(tag_ind+1, '_'.join(map(str, prev_tags))))))\n",
    "                \n",
    "            delta = list(range(hypo.pos-self.tagger_params.src_window, hypo.pos+self.tagger_params.src_window+1))\n",
    "            delta = list(filter(lambda x: (x != hypo.pos), delta))\n",
    "    \n",
    "            # delta_filtered = list(filter(lambda x: (x>=0 and x<len(self.source_sentence)\n",
    "            #                                and x != hypo.pos), delta))\n",
    "            \n",
    "            for ind in delta:\n",
    "                if ind >= 0:\n",
    "                    try:\n",
    "                        word_orig = self.source_sentence[ind]\n",
    "                        word = self.source_sentence[ind].lower()\n",
    "                        if (ind == 0):\n",
    "                            if word_orig.isupper():\n",
    "                                word = word_orig\n",
    "                        \n",
    "                    except IndexError:\n",
    "                        word = 'OOS'    \n",
    "                else:\n",
    "                    word = 'OOS'\n",
    "                    # out of sentence\n",
    "                features.append(h((hypo.tagged_word.tag, 'ind_{}_{}'.format(ind, word))))\n",
    "                \n",
    "            features = [h(elem) % self.tagger_params.nparams for elem in features]\n",
    "\n",
    "            # t_{i-1} - предыдущий тег\n",
    "            # t_{i-2}t_{i-1} - два предыдущий тега\n",
    "            # слова в окне - w_{i-1} - два вперёд и два назадъ\n",
    "            return Update(positions=np.array(features), values=np.ones(len(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part V: Beam search\n",
    "\n",
    "We can find the most likely tagging approximately using Beam Search. As everything else, it comes with a separate interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchTask:\n",
    "    \"\"\"\n",
    "    An abstract beam search task. Can be used with beam_search() generic\n",
    "    function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tagger_params, source_sentence, model, tags):\n",
    "        self._tagger_params = tagger_params\n",
    "        self._source_sentence = source_sentence\n",
    "        self._model = model\n",
    "        self._tags = tags\n",
    "        self._feature_computer = FeatureComputer(tagger_params, source_sentence)\n",
    "\n",
    "    def total_num_steps(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses between beginning and end (number of words in\n",
    "        the sentence).\n",
    "        \"\"\"\n",
    "        return len(self._source_sentence)\n",
    "\n",
    "    def beam_size(self):\n",
    "        return self._tagger_params.beam_size\n",
    "\n",
    "    def _get_next_hypo(self, hypo, tag):\n",
    "        pos = 0 if not hypo else hypo.pos + 1\n",
    "        tagged_word = TaggedWord(self._source_sentence[pos], tag)\n",
    "        new_hypo = Hypo(hypo, pos, tagged_word, 0)\n",
    "        features = self._feature_computer.compute_features(new_hypo)\n",
    "        score = self._model.score(features)\n",
    "        \n",
    "        if hypo:\n",
    "            score += hypo.score\n",
    "            \n",
    "        return new_hypo._replace(score=score)\n",
    "\n",
    "    def expand(self, hypo):\n",
    "        \"\"\"\n",
    "        Given Hypo, return a list of its possible expansions.\n",
    "        'hypo' might be None -- return a list of initial hypos then.\n",
    "        \n",
    "        Compute hypotheses' scores inside this function!\n",
    "        \"\"\"\n",
    "        return [self._get_next_hypo(hypo, tag) for tag in self._tags]\n",
    "\n",
    "    def recombo_hash(self, hypo):\n",
    "        \"\"\"\n",
    "        If two hypos have the same recombination hashes, they can be collapsed\n",
    "        together, leaving only the hypothesis with a better score.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        \n",
    "        for i in range(self._tagger_params.dst_order):\n",
    "            result.append(hypo.tagged_word.tag)\n",
    "            hypo = hypo.prev\n",
    "            \n",
    "        return result\n",
    "\n",
    "\n",
    "def beam_search(beam_search_task):\n",
    "    \"\"\"\n",
    "    Return list of stacks.\n",
    "    Each stack contains several hypos, sorted by score in descending \n",
    "    order (i.e. better hypos first).\n",
    "    \"\"\"\n",
    "    if not beam_search_task.total_num_steps():\n",
    "        return [[]]\n",
    "\n",
    "    beam_size = beam_search_task.beam_size()\n",
    "    first_stack = beam_search_task.expand(None)\n",
    "    stacks = [sorted(first_stack, key=lambda x: -x.score)[:beam_size]]\n",
    "\n",
    "    for _ in range(1, beam_search_task.total_num_steps()):\n",
    "        stack = []\n",
    "        for hypo in stacks[-1]:\n",
    "            stack += beam_search_task.expand(hypo)\n",
    "\n",
    "        stacks.append(sorted(stack, key=lambda x: -x.score)[:beam_size])\n",
    "\n",
    "    return stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(sentence, tagger_params, model, tags):\n",
    "        if not sentence:\n",
    "            return []\n",
    "        sentence = [word.text for word in sentence]\n",
    "        bst = BeamSearchTask(tagger_params, sentence, model, tags)\n",
    "        stacks = beam_search(bst)\n",
    "        hypo = stacks[-1][0]\n",
    "        result = []\n",
    "        for word in reversed(sentence):\n",
    "            result.append(TaggedWord(word, hypo.tagged_word.tag))\n",
    "            hypo = hypo.prev\n",
    "        return list(reversed(result))\n",
    "\n",
    "def tag_sentences(dataset, tagger_params, model, tags):\n",
    "    \"\"\"\n",
    "    Main predict function.\n",
    "    Tags all sentences in dataset. Dataset is a list of TaggedSentence; while \n",
    "    tagging, ignore existing tags.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [tag_sentence(sentence, tagger_params, model, tags) for sentence in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VI: Optimization objective and algorithm\n",
    "\n",
    "Once we defined our model and inference algorithm, we can define an optimization task: an object that computes loss function and its gradients w.r.t. model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationTask:\n",
    "    \"\"\"\n",
    "    Optimization task that can be used with sgd().\n",
    "    \"\"\"\n",
    "\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Parameters which are optimized in this optimization task.\n",
    "        Return Value.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        \"\"\"\n",
    "        Return (loss, gradient) on a specific example.\n",
    "\n",
    "        loss: float\n",
    "        gradient: Update\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "# class UnstructuredPerceptronOptimizationTask(OptimizationTask):\n",
    "#     def __init__(self, ...):\n",
    "#         <YOUR CODE>\n",
    "\n",
    "#     def params(self):\n",
    "#         <YOUR CODE>\n",
    "\n",
    "#     def loss_and_gradient(self, golden_sentence):\n",
    "#         <YOUR CODE>\n",
    "\n",
    "\n",
    "class StructuredPerceptronOptimizationTask(OptimizationTask):\n",
    "    def __init__(self, tagger_params, tags):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.model = LinearModel(tagger_params.nparams)\n",
    "        self.tags = tags\n",
    "        print(tags)\n",
    "\n",
    "    def params(self):\n",
    "        return self.model.params()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        # Do beam search.\n",
    "        beam_search_task = BeamSearchTask(\n",
    "            self.tagger_params,\n",
    "            [golden_tagged_word.text for golden_tagged_word in golden_sentence],\n",
    "            self.model,\n",
    "            self.tags\n",
    "            )\n",
    "        stacks = beam_search(beam_search_task)\n",
    "\n",
    "        # Compute chain of golden hypos (and their scores!).\n",
    "        golden_hypo = None\n",
    "        golden_source_sentence = [word.text for word in golden_sentence]\n",
    "        feature_computer = FeatureComputer(self.tagger_params, golden_source_sentence)\n",
    "        \n",
    "        max_violation = 0\n",
    "        for i, word in enumerate(golden_sentence):\n",
    "            new_golden_hypo = Hypo(golden_hypo, i, word, 0)\n",
    "            score = self.model.score(feature_computer.compute_features(new_golden_hypo))\n",
    "            if golden_hypo is not None:\n",
    "                score += golden_hypo.score\n",
    "\n",
    "            golden_hypo = new_golden_hypo._replace(score=score)\n",
    "\n",
    "            violation = stacks[i][-1].score - score\n",
    "            if violation > max_violation:\n",
    "                max_violation = violation\n",
    "                rival_head = stacks[i][0]\n",
    "                golden_head = golden_hypo\n",
    "\n",
    "        # Find where to update.\n",
    "        if max_violation == 0:\n",
    "            rival_head = stacks[-1][0]\n",
    "            golden_head = golden_hypo\n",
    "\n",
    "        # Compute gradient.\n",
    "        grad = Update()\n",
    "        while golden_head and rival_head:\n",
    "            for head, multiplyer in zip([rival_head, golden_head], [1, -1]):\n",
    "                features = feature_computer.compute_features(head)\n",
    "                add_grad = self.model.gradient(features, score=None)\n",
    "                grad.assign_madd(add_grad, multiplyer)\n",
    "\n",
    "            golden_head = golden_head.prev\n",
    "            rival_head = rival_head.prev\n",
    "\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VII: optimizer\n",
    "\n",
    "By this point we can define a model with parameters $\\theta$ and a problem that computes gradients $ \\partial L \\over \\partial \\theta $ w.r.t. model parameters.\n",
    "\n",
    "Optimization is performed by gradient descent: $ \\theta := \\theta - \\alpha {\\partial L \\over \\partial \\theta} $\n",
    "\n",
    "In order to speed up training, we use stochastic gradient descent that operates on minibatches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDParams = collections.namedtuple('SGDParams', [\n",
    "    'epochs',\n",
    "    'learning_rate',\n",
    "    'minibatch_size',\n",
    "    'average' # bool or int\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_batches(dataset, minibatch_size):\n",
    "    \"\"\"\n",
    "    Make list of batches from a list of examples.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    for begin_ind in range(0, len(dataset), minibatch_size):\n",
    "        batches.append(dataset[begin_ind:begin_ind + minibatch_size])\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def sgd(sgd_params, optimization_task, dataset, after_each_epoch_fn):\n",
    "    \"\"\"\n",
    "    Run (averaged) SGD on a generic optimization task. Modify optimization\n",
    "    task's parameters.\n",
    "\n",
    "    After each epoch (and also before and after the whole training),\n",
    "    run after_each_epoch_fn().\n",
    "    \"\"\"\n",
    "    after_each_epoch_fn()\n",
    "\n",
    "    if sgd_params.average:\n",
    "        params_sum = Value(len(optimization_task.params()))\n",
    "        added_cnt = 0\n",
    "\n",
    "    for _ in tqdm_notebook(range(sgd_params.epochs)):\n",
    "        batches = make_batches(dataset, sgd_params.minibatch_size)\n",
    "        for ind, batch in enumerate(batches):\n",
    "            grad = Update()\n",
    "            for sent in batch:\n",
    "                sent_grad = optimization_task.loss_and_gradient(sent)\n",
    "                grad.assign_madd(sent_grad, 1)\n",
    "            optimization_task.params().assign_madd(grad, -sgd_params.learning_rate)\n",
    "            if sgd_params.average and ind % sgd_params.average == sgd_params.average - 1:\n",
    "                params_sum.assign_madd(optimization_task.params(), 1)\n",
    "                added_cnt += 1\n",
    "        after_each_epoch_fn()\n",
    "\n",
    "    if sgd_params.average:\n",
    "        params_sum.assign_mul(1 / added_cnt)\n",
    "        optimization_task.params().assign(params_sum)\n",
    "        after_each_epoch_fn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VIII: Training loop\n",
    "\n",
    "The train function combines everthing you used below to get new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pickle\n",
    "\n",
    "def train(\n",
    "    tags='./data/tags',\n",
    "    train_dataset='./data/en-ud-train.conllu',\n",
    "    dev_dataset='./data/en-ud-dev.conllu',\n",
    "    model='./model.npz',\n",
    "    \n",
    "    sgd_epochs=15,\n",
    "    sgd_learning_rate=0.05,\n",
    "    sgd_minibatch_size=32,\n",
    "    sgd_average=True,\n",
    "    \n",
    "    # Number of context tags in output tagging to use for features\n",
    "    tagger_src_window=2,\n",
    "    \n",
    "    # Number of context tags in output tagging to use for features\n",
    "    tagger_dst_order=3,\n",
    "    \n",
    "    # Maximal number of prefix/suffix letters to use for features\n",
    "    tagger_max_suffix=4,\n",
    "    \n",
    "    # Width for beam search (0 means unstructured)\n",
    "    beam_size=4,\n",
    "    \n",
    "    # Parameter vector size (for hashing)\n",
    "    nparams= 2 * 22,\n",
    "):\n",
    "    \"\"\" Train a pos-tagger model and save it's parameters to :model: \"\"\"\n",
    "\n",
    "    with open(tags) as f:\n",
    "        tags = list(map(str.strip, f.readlines()))\n",
    "    # Beam size.\n",
    "    optimization_task_cls = StructuredPerceptronOptimizationTask\n",
    "    if beam_size == 0:\n",
    "        beam_size = 1\n",
    "        optimization_task_cls = UnstructuredPerceptronOptimizationTask\n",
    "\n",
    "    train_dataset = read_tagged_sentences(train_dataset)\n",
    "    dev_dataset = read_tagged_sentences(dev_dataset)\n",
    "    params = None\n",
    "    if os.path.exists(model):\n",
    "        params = pickle.load(open(model, 'rb'))\n",
    "    sgd_params = SGDParams(\n",
    "        epochs=sgd_epochs,\n",
    "        learning_rate=sgd_learning_rate,\n",
    "        minibatch_size=sgd_minibatch_size,\n",
    "        average=sgd_average\n",
    "        )\n",
    "    tagger_params = TaggerParams(\n",
    "        src_window=tagger_src_window,\n",
    "        dst_order=tagger_dst_order,\n",
    "        max_suffix=tagger_max_suffix,\n",
    "        beam_size=beam_size,\n",
    "        nparams=nparams\n",
    "        )\n",
    "\n",
    "    # Load optimization task\n",
    "    optimization_task = StructuredPerceptronOptimizationTask(tagger_params, tags)\n",
    "    if params is not None:\n",
    "        print('\\n\\nLoading parameters from %s\\n\\n' % model)\n",
    "        optimization_task.params().assign(params)\n",
    "\n",
    "    # Validation.\n",
    "    def after_each_epoch_fn():\n",
    "        lm = LinearModel(tagger_params.nparams)\n",
    "        lm.params().assign(optimization_task.params())\n",
    "        tagged_sentences = tag_sentences(dev_dataset, tagger_params, lm, tags)\n",
    "        q = pprint.pformat(tagging_quality(out=tagged_sentences, ref=dev_dataset))\n",
    "        print()\n",
    "        print(q)\n",
    "        print()\n",
    "\n",
    "        # Save parameters.\n",
    "        print('\\n\\nSaving parameters to %s\\n\\n' % model)\n",
    "        pickle.dump(optimization_task.params(), open(model, 'wb'))\n",
    "\n",
    "    # Run SGD.\n",
    "    sgd(sgd_params, optimization_task, train_dataset, after_each_epoch_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train a model with default params\n",
    "train(model='./model_beam_4.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IX: Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    tags='./data/tags',\n",
    "    dataset='./data/en-ud-dev.conllu',\n",
    "    model='./model.npz',\n",
    "    \n",
    "    # model and inference params; see train for their description\n",
    "    tagger_src_window=2,\n",
    "    tagger_dst_order=3,\n",
    "    tagger_max_suffix=4,\n",
    "    beam_size=1,\n",
    "):\n",
    "\n",
    "\n",
    "    tags = read_tags(tags)\n",
    "    dataset = read_tagged_sentences(dataset)\n",
    "    params = pickle.load(open(model, 'rb'))\n",
    "    tagger_params = TaggerParams(\n",
    "        src_window=tagger_src_window,\n",
    "        dst_order=tagger_dst_order,\n",
    "        max_suffix=tagger_max_suffix,\n",
    "        beam_size=beam_size,\n",
    "        nparams=0\n",
    "        )\n",
    "\n",
    "    # Load model.\n",
    "    model = LinearModel(params.values.shape[0])\n",
    "    model.params().assign(params)\n",
    "\n",
    "    # Tag all sentences.\n",
    "    tagged_sentences = tag_sentences(dataset, <YOUR_PARAMS>)\n",
    "\n",
    "    # Write tagged sentences.\n",
    "    for tagged_sentence in tagged_sentences:\n",
    "        write_tagged_sentence(tagged_sentence, sys.stdout)\n",
    "\n",
    "    # Measure and print quality.\n",
    "    q = pprint.pformat(tagging_quality(out=tagged_sentences, ref=dataset))\n",
    "    print(q, file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "test(model='./model_beam_4.npz')\n",
    "\n",
    "# sanity chec: accuracy > 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part X: play with it\n",
    "\n",
    "_This part is optional_\n",
    "\n",
    "Once you've built something, it's only natural to test the limits of your contraption.\n",
    "\n",
    "At minumum, we want you to find out how default model accuracy depends on __beam size__\n",
    "\n",
    "To get maximum points, your model should get final quality >= 93% \n",
    "\n",
    "Any further analysis is welcome, as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
