{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The grand quest: make it actually work (4 points)\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__. Try __at least 3 options__ from the list below for a passing grade. Write a short report about what you have tried. More ideas = more bonus points. \n",
    "\n",
    "__Please be serious:__ \" plot learning curves in MAE/epoch, compare models based on optimal performance, test one change at a time. You know the drill :)\n",
    "\n",
    "You can use either pure __tensorflow__ or __keras__. Feel free to adapt the seminar code for your needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.layers as L\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use some code from the seminar that contains preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
    "\n",
    "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
    "\n",
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "target_column = \"Log1pSalary\"\n",
    "\n",
    "data[categorical_columns] = data[categorical_columns].fillna('NaN')\n",
    "\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "data[\"FullDescription\"] = data[\"FullDescription\"].astype(str).apply(\n",
    "    lambda x: ' '.join(tokenizer.tokenize(x.lower())), 1)\n",
    "data[\"Title\"] = data[\"Title\"].astype(str).apply(\n",
    "    lambda x: ' '.join(tokenizer.tokenize(x.lower())), 1)\n",
    "\n",
    "tokens = []\n",
    "for title in data[\"Title\"]:\n",
    "    tokens += title.split()\n",
    "for title in data[\"FullDescription\"]:\n",
    "    tokens += title.split()\n",
    "    \n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "min_count = 10\n",
    "\n",
    "tokens = [token for token, count in token_counts.items() if count >= min_count]\n",
    "\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + sorted(tokens)\n",
    "token_to_id = {token: id for id, token in enumerate(tokens)}\n",
    "\n",
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
    "recognized_companies = set(top_companies)\n",
    "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "data_train.index = range(len(data_train))\n",
    "data_val.index = range(len(data_val))\n",
    "\n",
    "def make_batch(data, max_len=None, word_dropout=0):\n",
    "    \"\"\"\n",
    "    Creates a keras-friendly dict from the batch data.\n",
    "    :param word_dropout: replaces token index with UNK_IX with this probability\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len)\n",
    "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len)\n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if word_dropout != 0:\n",
    "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1. - word_dropout)\n",
    "    \n",
    "    if target_column in data.columns:\n",
    "        batch[target_column] = data[target_column].values\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
    "    dropout_mask &= matrix != pad_ix\n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, **kwargs):\n",
    "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch = make_batch(data.iloc[indices[start : start + batch_size]], **kwargs)\n",
    "            target = batch.pop(target_column)\n",
    "            yield batch, target\n",
    "        \n",
    "        if not cycle: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, data, batch_size=batch_size, name=\"\", **kw):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    for batch_x, batch_y in iterate_minibatches(data, batch_size=batch_size, shuffle=False, **kw):\n",
    "        batch_pred = model.predict(batch_x)[:, 0]\n",
    "        squared_error += np.sum(np.square(batch_pred - batch_y))\n",
    "        abs_error += np.sum(np.abs(batch_pred - batch_y))\n",
    "        num_samples += len(batch_y)\n",
    "    print(\"%s results:\" % (name or \"\"))\n",
    "    print(\"Mean square error: %.5f\" % (squared_error / num_samples))\n",
    "    print(\"Mean absolute error: %.5f\" % (abs_error / num_samples))\n",
    "    return squared_error, abs_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's define some consts and useful callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 100\n",
    "steps_per_epoch = 100\n",
    "n_tokens=len(tokens)\n",
    "n_cat_features=len(categorical_vectorizer.vocabulary_)\n",
    "hid_size=64\n",
    "\n",
    "callbacks = [\n",
    "    # Early stopping callback\n",
    "    keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', min_delta=0.0005, patience=3),\n",
    "    # Tensorboard to visualize learning curves\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs/')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the seminar model and look how strong its baseline is. Ofc we'll put our callbacks inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64):\n",
    "    l_title = L.Input(shape=[None], name=\"Title\")\n",
    "    l_descr = L.Input(shape=[None], name=\"FullDescription\")\n",
    "    l_categ = L.Input(shape=[n_cat_features], name=\"Categorical\")\n",
    "    \n",
    "    emb = L.Embedding(n_tokens, 2 * hid_size)\n",
    "    \n",
    "    l_title_emb = emb(l_title)\n",
    "    l_descr_emb = emb(l_descr)\n",
    "    \n",
    "    l_title_conv = L.Convolution1D(hid_size, kernel_size=2, activation='relu')(l_title_emb)\n",
    "    l_descr_conv = L.Convolution1D(hid_size, kernel_size=5, activation='relu')(l_descr_emb)\n",
    "    \n",
    "    l_title_out = L.GlobalMaxPool1D()(l_title_conv)\n",
    "    l_descr_out = L.GlobalMaxPool1D()(l_descr_conv)\n",
    "    \n",
    "    l_categ_out = L.Dense(hid_size, activation='relu')(l_categ)\n",
    "    \n",
    "    l_combined = L.Concatenate()([l_title_out, l_descr_out, l_categ_out])\n",
    "    l_dense_clf = L.Dense(hid_size, activation='relu')(l_combined)\n",
    "    \n",
    "    output_layer = L.Dense(1)(l_dense_clf)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[l_title, l_descr, l_categ], outputs=[output_layer])\n",
    "    model.compile('adam', 'mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 19s 192ms/step - loss: 22.5190 - mean_absolute_error: 3.0941 - val_loss: 0.3802 - val_mean_absolute_error: 0.4724\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 16s 158ms/step - loss: 0.2688 - mean_absolute_error: 0.3991 - val_loss: 0.2797 - val_mean_absolute_error: 0.4030\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 15s 148ms/step - loss: 0.1920 - mean_absolute_error: 0.3322 - val_loss: 0.1938 - val_mean_absolute_error: 0.3337\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.1502 - mean_absolute_error: 0.2919 - val_loss: 0.1553 - val_mean_absolute_error: 0.2982\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 13s 133ms/step - loss: 0.1299 - mean_absolute_error: 0.2727 - val_loss: 0.1263 - val_mean_absolute_error: 0.2669\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 13s 130ms/step - loss: 0.1190 - mean_absolute_error: 0.2572 - val_loss: 0.1126 - val_mean_absolute_error: 0.2514\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 13s 130ms/step - loss: 0.1065 - mean_absolute_error: 0.2437 - val_loss: 0.1054 - val_mean_absolute_error: 0.2427\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.0962 - mean_absolute_error: 0.2317 - val_loss: 0.0976 - val_mean_absolute_error: 0.2328\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0849 - mean_absolute_error: 0.2164 - val_loss: 0.0889 - val_mean_absolute_error: 0.2216\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.0824 - mean_absolute_error: 0.2147 - val_loss: 0.0849 - val_mean_absolute_error: 0.2166\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.0795 - mean_absolute_error: 0.2084 - val_loss: 0.0816 - val_mean_absolute_error: 0.2132\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.0761 - mean_absolute_error: 0.2047 - val_loss: 0.0778 - val_mean_absolute_error: 0.2063\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.0735 - mean_absolute_error: 0.2013 - val_loss: 0.0754 - val_mean_absolute_error: 0.2053\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.0723 - mean_absolute_error: 0.1998 - val_loss: 0.0718 - val_mean_absolute_error: 0.1981\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.0707 - mean_absolute_error: 0.1958 - val_loss: 0.0711 - val_mean_absolute_error: 0.1983\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.0602 - mean_absolute_error: 0.1809 - val_loss: 0.0686 - val_mean_absolute_error: 0.1934\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.0571 - mean_absolute_error: 0.1766 - val_loss: 0.0687 - val_mean_absolute_error: 0.1928\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.0592 - mean_absolute_error: 0.1785 - val_loss: 0.0673 - val_mean_absolute_error: 0.1923\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 10s 103ms/step - loss: 0.0593 - mean_absolute_error: 0.1775 - val_loss: 0.0672 - val_mean_absolute_error: 0.1911\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.0583 - mean_absolute_error: 0.1782 - val_loss: 0.0670 - val_mean_absolute_error: 0.1908\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.0560 - mean_absolute_error: 0.1746 - val_loss: 0.0635 - val_mean_absolute_error: 0.1848\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 10s 104ms/step - loss: 0.0567 - mean_absolute_error: 0.1749 - val_loss: 0.0646 - val_mean_absolute_error: 0.1884\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.0562 - mean_absolute_error: 0.1755 - val_loss: 0.0617 - val_mean_absolute_error: 0.1821\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 10s 104ms/step - loss: 0.0458 - mean_absolute_error: 0.1576 - val_loss: 0.0636 - val_mean_absolute_error: 0.1849\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.0451 - mean_absolute_error: 0.1564 - val_loss: 0.0615 - val_mean_absolute_error: 0.1823\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.0459 - mean_absolute_error: 0.1572 - val_loss: 0.0605 - val_mean_absolute_error: 0.1802\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.0470 - mean_absolute_error: 0.1598 - val_loss: 0.0608 - val_mean_absolute_error: 0.1819\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 10s 105ms/step - loss: 0.0479 - mean_absolute_error: 0.1609 - val_loss: 0.0646 - val_mean_absolute_error: 0.1885\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.0470 - mean_absolute_error: 0.1595 - val_loss: 0.0607 - val_mean_absolute_error: 0.1806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08d0becf60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.fit_generator(iterate_minibatches(data_train, batch_size, cycle=True, word_dropout=0.05), \n",
    "                    epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=iterate_minibatches(data_val, batch_size, cycle=True),\n",
    "                    validation_steps=data_val.shape[0] // batch_size,\n",
    "                    callbacks=callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results:\n",
      "Mean square error: 0.03584\n",
      "Mean absolute error: 0.13863\n",
      "Val results:\n",
      "Mean square error: 0.06084\n",
      "Mean absolute error: 0.18102\n"
     ]
    }
   ],
   "source": [
    "print_metrics(model, data_train, name='Train')\n",
    "print_metrics(model, data_val, name='Val');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve it, let's try to add more neurons in the dense layers and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.fit_generator(iterate_minibatches(data_train, batch_size, cycle=True, word_dropout=0.05), \n",
    "                        epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=iterate_minibatches(data_val, batch_size, cycle=True),\n",
    "                        validation_steps=data_val.shape[0] // batch_size,\n",
    "                        callbacks=callbacks\n",
    "                       )\n",
    "    \n",
    "    print_metrics(model, data_train, name='Train')\n",
    "    print_metrics(model, data_val, name='Val');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 16.0088 - mean_absolute_error: 2.4636 - val_loss: 0.3215 - val_mean_absolute_error: 0.4403\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 10s 104ms/step - loss: 0.3178 - mean_absolute_error: 0.4424 - val_loss: 0.1914 - val_mean_absolute_error: 0.3346\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.2357 - mean_absolute_error: 0.3774 - val_loss: 0.1632 - val_mean_absolute_error: 0.3085\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.1821 - mean_absolute_error: 0.3300 - val_loss: 0.1257 - val_mean_absolute_error: 0.2671\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.1499 - mean_absolute_error: 0.2975 - val_loss: 0.1107 - val_mean_absolute_error: 0.2490\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.1316 - mean_absolute_error: 0.2763 - val_loss: 0.1014 - val_mean_absolute_error: 0.2378\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 11s 105ms/step - loss: 0.1135 - mean_absolute_error: 0.2576 - val_loss: 0.0900 - val_mean_absolute_error: 0.2230\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 10s 105ms/step - loss: 0.1027 - mean_absolute_error: 0.2428 - val_loss: 0.0889 - val_mean_absolute_error: 0.2220\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 11s 105ms/step - loss: 0.0896 - mean_absolute_error: 0.2253 - val_loss: 0.0877 - val_mean_absolute_error: 0.2201\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 11s 105ms/step - loss: 0.0855 - mean_absolute_error: 0.2198 - val_loss: 0.0816 - val_mean_absolute_error: 0.2133\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.0807 - mean_absolute_error: 0.2137 - val_loss: 0.0775 - val_mean_absolute_error: 0.2072\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.0766 - mean_absolute_error: 0.2082 - val_loss: 0.0749 - val_mean_absolute_error: 0.2011\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.0754 - mean_absolute_error: 0.2049 - val_loss: 0.0725 - val_mean_absolute_error: 0.1982\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.0766 - mean_absolute_error: 0.2038 - val_loss: 0.0740 - val_mean_absolute_error: 0.2026\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.0720 - mean_absolute_error: 0.1981 - val_loss: 0.0695 - val_mean_absolute_error: 0.1943\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.0624 - mean_absolute_error: 0.1856 - val_loss: 0.0701 - val_mean_absolute_error: 0.1952\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.0598 - mean_absolute_error: 0.1813 - val_loss: 0.0713 - val_mean_absolute_error: 0.1992\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.0603 - mean_absolute_error: 0.1803 - val_loss: 0.0676 - val_mean_absolute_error: 0.1905\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 11s 105ms/step - loss: 0.0605 - mean_absolute_error: 0.1817 - val_loss: 0.0690 - val_mean_absolute_error: 0.1927\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 11s 105ms/step - loss: 0.0594 - mean_absolute_error: 0.1801 - val_loss: 0.0661 - val_mean_absolute_error: 0.1878\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 10s 104ms/step - loss: 0.0608 - mean_absolute_error: 0.1821 - val_loss: 0.0657 - val_mean_absolute_error: 0.1874\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 11s 105ms/step - loss: 0.0610 - mean_absolute_error: 0.1812 - val_loss: 0.0655 - val_mean_absolute_error: 0.1869\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.0619 - mean_absolute_error: 0.1825 - val_loss: 0.0802 - val_mean_absolute_error: 0.2171\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 0.0503 - mean_absolute_error: 0.1656 - val_loss: 0.0647 - val_mean_absolute_error: 0.1866\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.0505 - mean_absolute_error: 0.1665 - val_loss: 0.0646 - val_mean_absolute_error: 0.1856\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.0504 - mean_absolute_error: 0.1652 - val_loss: 0.0640 - val_mean_absolute_error: 0.1851\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.0518 - mean_absolute_error: 0.1671 - val_loss: 0.0649 - val_mean_absolute_error: 0.1872\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.0544 - mean_absolute_error: 0.1715 - val_loss: 0.0639 - val_mean_absolute_error: 0.1843\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.0520 - mean_absolute_error: 0.1686 - val_loss: 0.0629 - val_mean_absolute_error: 0.1827\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 10s 105ms/step - loss: 0.0523 - mean_absolute_error: 0.1680 - val_loss: 0.0626 - val_mean_absolute_error: 0.1814\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.0492 - mean_absolute_error: 0.1622 - val_loss: 0.0634 - val_mean_absolute_error: 0.1836\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.0438 - mean_absolute_error: 0.1530 - val_loss: 0.0631 - val_mean_absolute_error: 0.1829\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.0432 - mean_absolute_error: 0.1532 - val_loss: 0.0628 - val_mean_absolute_error: 0.1831\n",
      "Train results:\n",
      "Mean square error: 0.03653\n",
      "Mean absolute error: 0.13851\n",
      "Val results:\n",
      "Mean square error: 0.06312\n",
      "Mean absolute error: 0.18370\n"
     ]
    }
   ],
   "source": [
    "l_categ_out = L.Dense(2 * hid_size, activation='relu')(l_categ)\n",
    "l_categ_out = L.Dropout(0.5)(l_categ_out)\n",
    "\n",
    "l_combined = L.Concatenate()([l_title_out, l_descr_out, l_categ_out])\n",
    "l_dense_clf = L.Dense(2 * hid_size, activation='relu')(l_combined)\n",
    "\n",
    "output_layer = L.Dense(1)(l_dense_clf)\n",
    "\n",
    "model = keras.models.Model(inputs=[l_title, l_descr, l_categ], outputs=[output_layer])\n",
    "model.compile('adam', 'mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost no affect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we gonna separate convolutional/dense and activation layers and place a batchnorm in the midst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 68.5091 - mean_absolute_error: 8.1989 - val_loss: 17.8413 - val_mean_absolute_error: 4.0067\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 28.1971 - mean_absolute_error: 5.1998 - val_loss: 3.9842 - val_mean_absolute_error: 1.6866\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 7.3154 - mean_absolute_error: 2.5457 - val_loss: 0.5466 - val_mean_absolute_error: 0.5697\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 1.1929 - mean_absolute_error: 0.9159 - val_loss: 1.0370 - val_mean_absolute_error: 0.8508\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.3582 - mean_absolute_error: 0.4681 - val_loss: 2.2258 - val_mean_absolute_error: 1.4149\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.2727 - mean_absolute_error: 0.4066 - val_loss: 2.0186 - val_mean_absolute_error: 1.3514\n",
      "Train results:\n",
      "Mean square error: 2.00679\n",
      "Mean absolute error: 1.35711\n",
      "Val results:\n",
      "Mean square error: 2.01609\n",
      "Mean absolute error: 1.35066\n"
     ]
    }
   ],
   "source": [
    "l_title_conv = L.Convolution1D(hid_size, kernel_size=2)(l_title_emb)\n",
    "l_title_conv = L.BatchNormalization()(l_title_conv)\n",
    "l_title_conv = L.Activation('relu')(l_title_conv)\n",
    "l_descr_conv = L.Convolution1D(hid_size, kernel_size=5)(l_descr_emb)\n",
    "l_descr_conv = L.BatchNormalization()(l_descr_conv)\n",
    "l_descr_conv = L.Activation('relu')(l_descr_conv)\n",
    "\n",
    "l_title_out = L.GlobalMaxPool1D()(l_title_conv)\n",
    "l_descr_out = L.GlobalMaxPool1D()(l_descr_conv)\n",
    "\n",
    "l_categ_out = L.Dense(hid_size, activation='relu')(l_categ)\n",
    "l_categ_out = L.BatchNormalization()(l_categ_out)\n",
    "l_categ_out = L.Activation('relu')(l_categ_out)\n",
    "\n",
    "l_combined = L.Concatenate()([l_title_out, l_descr_out, l_categ_out])\n",
    "\n",
    "l_dense_clf = L.Dense(hid_size, activation='relu')(l_combined)\n",
    "l_dense_clf = L.BatchNormalization()(l_dense_clf)\n",
    "l_dense_clf = L.Activation('relu')(l_dense_clf)\n",
    "\n",
    "output_layer = L.Dense(1)(l_dense_clf)\n",
    "\n",
    "model = keras.models.Model(inputs=[l_title, l_descr, l_categ], outputs=[output_layer])\n",
    "model.compile('adam', 'mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we always can add more layers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 7.4275 - mean_absolute_error: 1.5666 - val_loss: 0.6399 - val_mean_absolute_error: 0.7085\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.4719 - mean_absolute_error: 0.5333 - val_loss: 0.6582 - val_mean_absolute_error: 0.7421\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.3756 - mean_absolute_error: 0.4753 - val_loss: 0.7919 - val_mean_absolute_error: 0.8313\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.2137 - mean_absolute_error: 0.3496 - val_loss: 0.2379 - val_mean_absolute_error: 0.4037\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.1518 - mean_absolute_error: 0.2843 - val_loss: 0.1516 - val_mean_absolute_error: 0.2999\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.1468 - mean_absolute_error: 0.2815 - val_loss: 0.1380 - val_mean_absolute_error: 0.2815\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.1471 - mean_absolute_error: 0.2757 - val_loss: 0.1009 - val_mean_absolute_error: 0.2170\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.1295 - mean_absolute_error: 0.2579 - val_loss: 0.1181 - val_mean_absolute_error: 0.2426\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 0.1578 - mean_absolute_error: 0.2932 - val_loss: 0.4002 - val_mean_absolute_error: 0.5694\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.0949 - mean_absolute_error: 0.2161 - val_loss: 0.0916 - val_mean_absolute_error: 0.2082\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 0.1178 - mean_absolute_error: 0.2495 - val_loss: 0.1083 - val_mean_absolute_error: 0.2376\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.1368 - mean_absolute_error: 0.2610 - val_loss: 0.1475 - val_mean_absolute_error: 0.2941\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.1150 - mean_absolute_error: 0.2416 - val_loss: 0.1311 - val_mean_absolute_error: 0.2751\n",
      "Train results:\n",
      "Mean square error: 0.11189\n",
      "Mean absolute error: 0.25803\n",
      "Val results:\n",
      "Mean square error: 0.12985\n",
      "Mean absolute error: 0.27466\n"
     ]
    }
   ],
   "source": [
    "l_categ_out = L.Dense(hid_size, activation='relu')(l_categ)\n",
    "l_categ_out = L.Dense(hid_size, activation='relu')(l_categ_out)\n",
    "\n",
    "l_combined = L.Concatenate()([l_title_out, l_descr_out, l_categ_out])\n",
    "l_dense_clf = L.Dense(2 * hid_size, activation='relu')(l_combined)\n",
    "l_dense_clf = L.Dense(2 * hid_size, activation='relu')(l_dense_clf)\n",
    "\n",
    "output_layer = L.Dense(1)(l_dense_clf)\n",
    "\n",
    "model = keras.models.Model(inputs=[l_title, l_descr, l_categ], outputs=[output_layer])\n",
    "model.compile('adam', 'mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is quite unstable..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortunately, I needed to go to sleep :((("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short report\n",
    "\n",
    "Please tell us what you did and how did it work.\n",
    "\n",
    "`<YOUR_TEXT_HERE>`, i guess..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended options\n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `L.BatchNormalization`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to perform pooling:\n",
    "* Max over time - our `L.GlobalMaxPool1D`\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
    "\n",
    ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a dense layer.\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$ (aka multi-headed attention).\n",
    "\n",
    "The catch is that keras layers do not inlude those toys. You will have to [write your own keras layer](https://keras.io/layers/writing-your-own-keras-layers/). Or use pure tensorflow, it might even be easier :)\n",
    "\n",
    "#### C) Fun with words\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained embeddings from `gensim.downloader.load`. See last lecture.\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent. You may or may not want to use __`.get_keras_embedding()`__ method for word2vec\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [early stopping callback](https://keras.io/callbacks/#earlystopping).\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.save(file_name)`\n",
    "  * Plotting learning curves is usually a good idea\n",
    "  \n",
    "Good luck! And may the force be with you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
