{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word Alignment Assignment__\n",
    "\n",
    "Your task is to learn word alignments for the data provided with this Python Notebook. \n",
    "\n",
    "Start by running the 'train' function below and implementing the assertions which will fail. Then consider the following improvements to the baseline model:\n",
    "* Is the TranslationModel parameterized efficiently?\n",
    "* What form of PriorModel would help here? (Currently the PriorModel is uniform.)\n",
    "* How could you use a Hidden Markov Model to model word alignment indices? (There's an implementation of simple HMM below to help you start.)\n",
    "* How could you initialize more complex models from simpler ones?\n",
    "* How could you model words that are not aligned to anything?\n",
    "\n",
    "Grades will be assigned as follows*:\n",
    "\n",
    " AER below on blinds   |  Grade \n",
    "----------|-------------\n",
    " 0.5 - 0.6 |   1 \n",
    " 0.4 - 0.5 |   2 \n",
    " 0.35 - 0.4 |  3    \n",
    " 0.3 - 0.35 |  4    \n",
    " 0.25 - 0.3 |  5   \n",
    " \n",
    "You should save the notebook with the final scores for 'dev' and 'test' test sets.\n",
    "\n",
    "*__Note__: Students who submitted a version of this assignment last year will have a 0.05 AER handicap, i.e to get a grade of 5, they will need to get an AER below 0.25.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the generative models that you may want to use for word alignment.\n",
    "# Currently only the TranslationModel is at all functional.\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, src_corpus, trg_corpus):\n",
    "        self._probs = defaultdict(lambda: defaultdict(lambda: 1))\n",
    "        self._counter = defaultdict(lambda: defaultdict(lambda: 0))        \n",
    "\n",
    "    def recompute_parameters(self):\n",
    "        for first_key, inner_dict in self._counter.items():\n",
    "            total_count = 1e-7 * len(inner_dict) + sum(list(inner_dict.values()))\n",
    "            \n",
    "            for second_key, cnt in inner_dict.items():\n",
    "                self._probs[first_key][second_key] = (1e-7 + cnt) / total_count\n",
    "\n",
    "        self._counter = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "\n",
    "class TranslationModel(Model):\n",
    "    \"Models conditional distribution over trg words given a src word.\"\n",
    "    \n",
    "    def __init__(self, src_corpus, trg_corpus):\n",
    "        super().__init__(src_corpus, trg_corpus)\n",
    "        for s_sent, t_sent in zip(src_corpus, trg_corpus):\n",
    "            for s_word in s_sent:\n",
    "                for t_word in t_sent:\n",
    "                    self._counter[s_word][t_word] += 1\n",
    "                    \n",
    "        self.recompute_parameters()\n",
    "\n",
    "    def get_conditional_prob(self, src_token, trg_token):\n",
    "        \"Return the conditional probability of trg_token given src_token.\"\n",
    "        return self._probs[src_token][trg_token]\n",
    "\n",
    "    def get_parameters_for_sentence_pair(self, src_tokens, trg_tokens):\n",
    "        \"Returns matrix with t[i][j] = p(f_j|e_i).\"\n",
    "        return np.array([[self.get_conditional_prob(src_token, trg_token)\n",
    "                          for trg_token in trg_tokens] for src_token in src_tokens])\n",
    "\n",
    "    def collect_statistics(self, src_tokens, trg_tokens, posterior_matrix):\n",
    "        \"Accumulate counts of translations from: posterior_matrix[j][i] = p(a_j=i|e, f)\"\n",
    "        for i, src_token in enumerate(src_tokens):\n",
    "            for j, trg_token in enumerate(trg_tokens):\n",
    "                self._counter[src_token][trg_token] += posterior_matrix[j, i]\n",
    "\n",
    "\n",
    "class TransitionModel(Model):\n",
    "    \"Models the prior probability of an alignment conditioned on previous alignment.\"\n",
    "\n",
    "    def get_parameters_for_sentence_pair(self, src_length):\n",
    "        \"Retrieve the parameters for this sentence pair: A[k, i] = p(a_{j} = i|a_{j-1} = k)\"\n",
    "        transition = np.array([[self._probs[(src_length)][i - k] for i in range(src_length)]\n",
    "                               for k in range(src_length)])\n",
    "        initial = np.array([self._probs[(src_length, 'initial')][i] for i in range(src_length)])\n",
    "        \n",
    "        return initial, transition\n",
    "\n",
    "    def collect_statistics(self, src_length, posteriors, single_posteriors):\n",
    "        \"Extract statistics from the bigram posterior[i][j]: p(a_{t-1} = i, a_{t} = j| e, f)\"\n",
    "        for i in range(src_length):\n",
    "            self._counter[(src_length, 'initial')][i] += single_posteriors[0, i]\n",
    "            for k in range(src_length):\n",
    "                self._counter[(src_length)][i - k] += posteriors[:, k, i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ans(pi, A, O):\n",
    "    probs = np.zeros_like(O)\n",
    "    max_path = np.zeros_like(probs, dtype=np.int8)\n",
    "    \n",
    "    for t in range(O.shape[1]):\n",
    "        if t:\n",
    "            possible_probs = probs[:, t - 1] + (np.log(O[:, t]) + np.log(A)).T\n",
    "            probs[:, t] = np.max(possible_probs, axis=1)\n",
    "            max_path[:, t] = np.argmax(possible_probs, axis=1)\n",
    "        else:\n",
    "            probs[:, t] = np.log(pi) + np.log(O[:, t])\n",
    "\n",
    "    result = [np.argmax(probs[:, -1])]\n",
    "    \n",
    "    for i in range(O.shape[1] - 1, 0, -1):\n",
    "        result.append(max_path[result[-1], i])\n",
    "        \n",
    "    return np.array(list(reversed(result)))\n",
    "\n",
    "def forward(pi, A, O, observations):\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "\n",
    "    alpha = np.zeros((N, S))\n",
    "    alpha[0] = pi * O[:, observations[0]]\n",
    "\n",
    "    for i in range(1, N):\n",
    "        alpha[i] = O[:, observations[i]] * np.sum(A.T * alpha[i - 1], axis=1)\n",
    "        \n",
    "    return alpha\n",
    "\n",
    "\n",
    "def backward(pi, A, O, observations):\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "\n",
    "    beta = np.zeros((N, S))\n",
    "    beta[N - 1] = 1\n",
    "    \n",
    "    for i in range(N - 2, -1, -1):\n",
    "        beta[i] = np.sum(O[:, observations[i + 1]] * A * beta[i + 1], axis=1)\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the framework for training and evaluating a model using EM.\n",
    "\n",
    "from utils import read_parallel_corpus, extract_test_set_alignments, score_alignments, write_aligned_corpus\n",
    "\n",
    "def infer_posteriors(src_tokens, trg_tokens, transition_model, translation_model):\n",
    "    \"Compute the posterior probability p(a_j=i | f, e) for each target token f_j given e and f.\"\n",
    "    \n",
    "    initial, transition = transition_model.get_parameters_for_sentence_pair(len(src_tokens))\n",
    "    translation = translation_model.get_parameters_for_sentence_pair(src_tokens, trg_tokens)\n",
    "    params = (initial, transition, translation)\n",
    "\n",
    "    observations = np.arange(len(trg_tokens))\n",
    "    posteriors = np.zeros((len(trg_tokens) - 1, len(src_tokens), len(src_tokens)))\n",
    "    single_posteriors = np.zeros((len(trg_tokens), len(src_tokens)))\n",
    "    \n",
    "    alpha, beta = forward(*params, observations), backward(*params, observations)\n",
    "    answers = get_ans(*params)\n",
    "\n",
    "    for t in range(len(trg_tokens) - 1):\n",
    "        aggr = (alpha[t, :] * transition.T).T * translation[:, t + 1] * beta[t + 1, :]\n",
    "        posteriors[t] = aggr / np.sum(aggr)\n",
    "\n",
    "    aggr = alpha * beta\n",
    "    single_posteriors = (aggr.T / np.sum(aggr, axis=1)).T\n",
    "\n",
    "    log_likelihood = (np.log(initial[answers[0]]) +\n",
    "                      np.sum(np.log(transition[answers[:-1], answers[1:]])) +\n",
    "                      np.sum(np.log(translation[answers, np.arange(len(trg_tokens))])))\n",
    "    \n",
    "    return (posteriors, single_posteriors), log_likelihood\n",
    "\n",
    "def collect_expected_statistics(src_corpus, trg_corpus, transition_model, translation_model):\n",
    "    \"E-step: infer posterior distribution over each sentence pair and collect statistics.\"\n",
    "    corpus_log_likelihood = 0.0\n",
    "    for src_tokens, trg_tokens in zip(src_corpus, trg_corpus):\n",
    "        # Infer posterior\n",
    "        posteriors, log_likelihood = infer_posteriors(src_tokens, trg_tokens, transition_model, translation_model)\n",
    "        # Collect statistics in each model.\n",
    "        transition_model.collect_statistics(len(src_tokens), *posteriors)\n",
    "        translation_model.collect_statistics(src_tokens, trg_tokens, posteriors[1])\n",
    "        # Update log prob\n",
    "        corpus_log_likelihood += log_likelihood\n",
    "    return corpus_log_likelihood\n",
    "\n",
    "def estimate_models(src_corpus, trg_corpus, transition_model, translation_model, num_iterations):\n",
    "    \"Estimate models iteratively using EM.\"\n",
    "    for iteration in range(num_iterations):\n",
    "        # E-step\n",
    "        corpus_log_likelihood = collect_expected_statistics(\n",
    "            src_corpus, trg_corpus, transition_model, translation_model)\n",
    "        # M-step\n",
    "        transition_model.recompute_parameters()\n",
    "        translation_model.recompute_parameters()\n",
    "        if iteration > 0:\n",
    "            print(\"corpus log likelihood: %1.3f\" % corpus_log_likelihood)\n",
    "    return transition_model, translation_model\n",
    "\n",
    "def get_alignments_from_posterior(posteriors):\n",
    "    \"Returns the MAP alignment for each target word given the posteriors.\"\n",
    "    # HINT: If you implement an HMM, you may want to implement a better algorithm here.\n",
    "    alignments = {}\n",
    "    for src_index, trg_index in enumerate(np.argmax(posteriors[1], 1)):\n",
    "        if src_index not in alignments:\n",
    "            alignments[src_index] = {}\n",
    "        alignments[src_index][trg_index] = '*'\n",
    "    return alignments\n",
    "\n",
    "def align_corpus(src_corpus, trg_corpus, transition_model, translation_model):\n",
    "    \"Align each sentence pair in the corpus in turn.\"\n",
    "    aligned_corpus = []\n",
    "    for src_tokens, trg_tokens in zip(src_corpus, trg_corpus):\n",
    "        posteriors, _ = infer_posteriors(src_tokens, trg_tokens, transition_model, translation_model)\n",
    "        alignments = get_alignments_from_posterior(posteriors)\n",
    "        aligned_corpus.append((src_tokens, trg_tokens, alignments))\n",
    "    return aligned_corpus\n",
    "\n",
    "def initialize_models(src_corpus, trg_corpus):\n",
    "    transition_model = TransitionModel(src_corpus, trg_corpus)\n",
    "    translation_model = TranslationModel(src_corpus, trg_corpus)\n",
    "    return transition_model, translation_model\n",
    "\n",
    "def _normalize(corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        for i, word in enumerate(sentence):\n",
    "            word = lemmatizer.lemmatize(word.lower())\n",
    "            sentence[i] = word[:4]\n",
    "    return corpus\n",
    "    \n",
    "def normalize(*corpuses):\n",
    "    return map(_normalize, corpuses)\n",
    "\n",
    "def train(num_iterations):\n",
    "    src_corpus, trg_corpus, _ = read_parallel_corpus('en-cs.all')\n",
    "    src_corpus, trg_corpus = normalize(src_corpus, trg_corpus)\n",
    "    transition_model, translation_model = initialize_models(src_corpus, trg_corpus)\n",
    "    transition_model, translation_model = estimate_models(\n",
    "        src_corpus, trg_corpus, transition_model, translation_model, num_iterations)    \n",
    "    aligned_corpus = align_corpus(src_corpus, trg_corpus, transition_model, translation_model)\n",
    "    return aligned_corpus, extract_test_set_alignments(aligned_corpus)\n",
    "\n",
    "def evaluate(candidate_alignments):\n",
    "    src_dev, trg_dev, wa_dev = read_parallel_corpus('en-cs-wa.dev', has_alignments=True)\n",
    "    src_test, trg_test, wa_test = read_parallel_corpus('en-cs-wa.test', has_alignments=True)\n",
    "    print('recall %1.3f; precision %1.3f; aer %1.3f' % score_alignments(wa_dev, candidate_alignments['dev']))\n",
    "    print('recall %1.3f; precision %1.3f; aer %1.3f' % score_alignments(wa_test, candidate_alignments['test']))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1565618.707\n",
      "corpus log likelihood: -1217889.079\n",
      "corpus log likelihood: -1011286.348\n",
      "corpus log likelihood: -927266.082\n",
      "corpus log likelihood: -896550.879\n",
      "corpus log likelihood: -882508.412\n",
      "corpus log likelihood: -874757.727\n",
      "corpus log likelihood: -869847.180\n",
      "corpus log likelihood: -866582.785\n",
      "recall 0.698; precision 0.645; aer 0.331\n",
      "recall 0.693; precision 0.633; aer 0.340\n"
     ]
    }
   ],
   "source": [
    "aligned_corpus, test_alignments = train(10)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
